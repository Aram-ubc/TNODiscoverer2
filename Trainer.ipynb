{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9401b739-2fe2-4331-9fd4-cbabec10a0ec",
   "metadata": {},
   "source": [
    "## Training a TNO detecting CNN model\n",
    "\n",
    "**Author:** Aram Lee, Hossen Teimoorinia\n",
    "**Date:** 2025-02-02\n",
    "**File Name:** Trainer.ipynb\n",
    "\n",
    "### [Description]\n",
    "Using sub-images which contain artificial TNOs, a CNN model is trained to detect the TNOs.\n",
    "\n",
    "### [Required Libraries]\n",
    "- numpy: 1.26.4\n",
    "- astropy: 6.1.0\n",
    "- scikit-learn: 1.1.1 (sklearn)\n",
    "- tensorflow: 2.9.1\n",
    "\n",
    "### [Workflow]  \n",
    "\n",
    "Steps 1-3 are for training the model, and steps 4-6 are for using the model to detect TNOs.\n",
    "\n",
    "|Step|File|Input|Output|Purpose|\n",
    "|-|-|-|-|-|\n",
    "|1|ImageCutter.ipynb|.fits (with artificial moving objects), .plantlist (artificial objects info)| .npy|Extract sub-images for training|\n",
    "|2|Concatenator.ipynb|.npy (sub-images from ImageCutter)|.npy|Prepare dataset for training|\n",
    "|3|Trainer.ipynb **(Here)**|.npy (dataset from Concatenator), .npy (target information)|.h5 (trained CNN models)|Train the model|\n",
    "|-|-|-|-|-|\n",
    "|4|ImageCutter.ipynb|.fits (without artificial moving objects)|.npy|Extract sub-images for detection|\n",
    "|5|Predictor.ipynb|.npy (sub-images from ImageCutter), .npy (target info), .h5 (model)|.npy|Apply trained model to detect objects|\n",
    "|6a|Link_sources_to_objects.py|.npy (classification and regression output from Predictor)|.npy|Detect moving objects (linear fitting method)|\n",
    "|6b|CandidateFinder.ipynb|.npy (classification output from Predictor), .npy (sub-images, target info)|.csv|Detect moving objects (scoring method)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76a63f32-0afe-4147-8c3f-badce721d0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages to train CNNs\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "from math import ceil\n",
    "\n",
    "# Tensorflow is used to build CNNs, train them, and analyze the performance.\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "from tensorflow.keras.metrics import Recall\n",
    "from tensorflow.keras.metrics import Precision\n",
    "# tensorflow.compat.v1.keras.layers.enable_v2_dtype_behavior()\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "# Tensorflow keep the channels AFTER the rows and columns. We change it to \"channels first\".\n",
    "tensorflow.keras.backend.set_image_data_format('channels_first')\n",
    "\n",
    "# Use the mixed precision training for better performance.\n",
    "tensorflow.keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e04b8a3-e2bf-4489-b16c-d4f8a4c739d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM memory % used: 35.3 241 156\n"
     ]
    }
   ],
   "source": [
    "# Print the percentage of RAM used, total RAM (in GB), and available RAM (in GB) \n",
    "import psutil\n",
    "print('RAM memory % used:', psutil.virtual_memory()[2], psutil.virtual_memory()[0]>>30, psutil.virtual_memory()[1]>>30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "495b5a32-68bc-43d4-a4ca-dcb3709ad379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pre-cut images and their labels\n",
    "M_img_train = np.load('trainingsets/inp_ch051020_P99NN.npy', allow_pickle=True)\n",
    "M_tar_train = np.load('trainingsets/tar_ch051020_P99NN.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaf3d138-63e7-4dd8-bcd0-4cc6bb99def0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1,\n",
       "       list([46.784159935019346, 31.566126532308388, 233.71, 83.58, array(218.92584006), array(84.01387347)]),\n",
       "       list([42.555362777862655, 31.88228668748326, 228.22, 84.32, array(217.66463722), array(84.43771331)]),\n",
       "       'calexp-2426932-05-P99NN', 'calexp-2426940-05-P99NN',\n",
       "       array([ 6.48050000e+04,  2.33710000e+02,  8.35800000e+01,  1.43000000e+00,\n",
       "               6.61000000e+00, -1.53000000e+00, -1.70000000e-01,  2.62500000e+01,\n",
       "               2.18475310e-01,  2.89588904e+02, -2.14610940e+01])                ,\n",
       "       array([ 6.48050000e+04,  2.28220000e+02,  8.43200000e+01,  1.45000000e+00,\n",
       "               6.55000000e+00, -1.55000000e+00, -1.70000000e-01,  2.62500000e+01,\n",
       "               2.64396544e-01,  2.89588669e+02, -2.14611200e+01])                ],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Example of information for an image pair:\n",
    "\n",
    "- Index 0-1: Labels for classification.\n",
    "- Index 2-3: Lists containing positional values of the moving object:\n",
    "  [x_sub, y_sub, x, y, center_of_the_cutout_x, center_of_the_cutout_y].\n",
    "- Index 4-5: Original filenames of the sub-images.\n",
    "- Index 6-7: Lists from the plantlist files for the moving object in each sub-image:\n",
    "  [ID, x, y, rate, angle, rate_x, rate_y, magnitude, psf_flux_multiplier, ra, dec].\n",
    "\"\"\"\n",
    "M_tar_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71aa3252-03f0-4974-9b5d-d762899791eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following codes create (design) the ready-made MobileNet architecture and compile (choose the optimizer, loss, and metrics) the MobileNet.\n",
    "\n",
    "model_MobileNet_cls = tensorflow.keras.applications.MobileNet(\n",
    "    include_top=True,\n",
    "    weights=None,\n",
    "    input_tensor=None,\n",
    "    input_shape=(4,64,64),\n",
    "    pooling=None,\n",
    "    classes=2,\n",
    "    classifier_activation=\"sigmoid\"\n",
    ")\n",
    "\n",
    "opt = tensorflow.keras.optimizers.Adam()\n",
    "model_MobileNet_cls.compile(optimizer=opt, loss='binary_crossentropy', metrics=['acc', tensorflow.keras.metrics.BinaryAccuracy(), Recall(), Precision(), tensorflow.keras.metrics.TruePositives(), tensorflow.keras.metrics.TrueNegatives(), tensorflow.keras.metrics.FalsePositives(), tensorflow.keras.metrics.FalseNegatives()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "742e2ad0-6138-4364-ad28-b427a3bd35b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute the classfication model training process\n",
    "\n",
    "# batch size is the number of training examples utilized in one iteration.\n",
    "# epochs is the number of passes through the entire training dataset\n",
    "batch_size = 1024\n",
    "epochs = 30\n",
    "\n",
    "# Early Stopping stops training if val_loss does not improve for a set number of consecutive epochs.\n",
    "# It restores the best model weights before stopping.\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
    "\n",
    "# This is the actual fitting part.\n",
    "# validation_data: The loss on this data set is evaluated at the end of each epoch. This validation data is not used for training, only for monitoring performance.\n",
    "# shuffle: shuffles the training data in batch-sized chunks to improve generalization.\n",
    "# callbacks: functions that execute during training, such as early stopping.\n",
    "history_MobileNet_cls=model_MobileNet_cls.fit(M_img_train, M_tar_s_train[:,0:2], validation_data=(M_img_test, M_tar_s_test[:,0:2]),\n",
    "          shuffle=True, epochs=epochs, batch_size=batch_size, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9993d14-1bae-47b8-9d61-f03afa05e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained model\n",
    "model_MobileNet_cls.save('models/M_cls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df746c70-a47f-4cdb-8683-37611d07b5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the target files into Pandas DataFrames with labeled columns for easier magnitude-based filtering of TNOs.\n",
    "import pandas as pd\n",
    "dftrain = pd.DataFrame(M_tar_s_train, columns = ['p1', 'p2', 'x1', 'y1', 'x2', 'y2', 'm1', 'm2'])\n",
    "dftest = pd.DataFrame(M_tar_s_test, columns = ['p1', 'p2', 'x1', 'y1', 'x2', 'y2', 'm1', 'm2'])\n",
    "# an example of filtering\n",
    "dftrain11 = dftrain.index[((df.m1 <= 23) & (df.m1 > 0) & (df.m2 <= 23) & (df.m2 > 0))].tolist()\n",
    "dftest11 = dftest.index[((df.m1 <= 23) & (df.m1 > 0) & (df.m2 <= 23) & (df.m2 > 0))].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937f467c-3fa2-4e0e-b87b-f27725e4845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following codes define and compile the regression model for predicting positions and magnitudes of TNOs.\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.applications import MobileNet, MobileNetV2, ResNet50, ResNet50V2\n",
    "\n",
    "base_model = tensorflow.keras.applications.MobileNet(\n",
    "    input_shape=(2,64,64),\n",
    "    include_top=False,\n",
    "    weights=None,\n",
    "    input_tensor=None,\n",
    "    pooling=None,\n",
    "    classifier_activation=\"linear\"\n",
    ")\n",
    "\n",
    "x = base_model.output\n",
    "# flat_cnn = Flatten(name='flat1')(x)\n",
    "flat_cnn = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# two separate output branches for position and magnitude regression\n",
    "# (x_1st_image, y_1st_image, x_2nd_image, y_2nd_image)\n",
    "fc1 = Dropout(0)(flat_cnn)\n",
    "output_pos = Dense(4, activation='linear')(fc1)\n",
    "# (magnitude_1st_image, magnitude_2nd_image)\n",
    "fc2 = Dropout(0)(flat_cnn)\n",
    "output_mag = Dense(2, activation='linear')(fc2)\n",
    "\n",
    "# Define the model with two outputs\n",
    "model_MobileNet_rgs = Model(inputs=base_model.input, outputs=[output_pos, output_mag])\n",
    "\n",
    "# Compile the model with a mean absolute error loss function\n",
    "opt = tensorflow.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model_MobileNet_rgs.compile(optimizer=opt, loss='mae', metrics=['mae','mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cb7ed66-5e0c-48f1-a4e3-ba194efa1a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute the regression model training process\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
    "    \n",
    "batch_size = 1024\n",
    "epochs = 30\n",
    "\n",
    "history_MobileNet_rgs=model_MobileNet_rgs.fit(M_img_train[dftrain11], [M_tar_s_train[:,2:6][dftrain11], M_tar_s_train[:,6:8][dftrain11]], validation_data=(M_img_test[dftest11], [M_tar_s_test[:,2:6][dftest11], M_tar_s_test[:,6:8][dftest11]]),\n",
    "          shuffle=True, epochs=epochs, batch_size=batch_size, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cecc524-1902-4939-b452-20515c4ec19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MobileNet_rgs.save('models/M_rgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757dab4e-0937-48e8-b0b5-00892537f97f",
   "metadata": {},
   "source": [
    "**Following cells test the models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189f88bc-0f76-4181-ba23-4f7cb5c5aa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test, load the selected models used in the paper\n",
    "model_cls = tensorflow.keras.models.load_model('models/MobileNet_Ch051020M25_classification')\n",
    "model_rgs = tensorflow.keras.models.load_model('models/MobileNet_Ch051020M25_regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aea90a-959d-423d-b80a-3b07bab6dcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples of filtering, to make a balanced data set\n",
    "dft11 = df.index[((df.m1 <= 23) & (df.m1 > 0) & (df.m2 <= 23) & (df.m2 > 0))].tolist()\n",
    "dft00 = df.index[((df.m1 == -1) & (df.m2 == -1))].tolist()[0:len(dft11)]\n",
    "dft10 = df.index[((df.m1 <= 23) & (df.m2 == -1))].tolist()[0:ceil(len(dft11)/4)]\n",
    "dft01 = df.index[((df.m1 == -1) & (df.m2 <= 23))].tolist()[0:ceil(len(dft11)/4)]\n",
    "# concatenate the necessary filtered entries\n",
    "dfindex = np.concatenate((dft11,dft10,dft01,dft00))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7c5faa-af1d-4ed2-bacc-df75762d807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the trained classfication model\n",
    "pred_bin = model_MobileNet_cls.predict(M_img_test[dfindex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01db9342-cc24-4ddb-a8e7-d08cbf7e0e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix using the output from the classfication model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support as prfs\n",
    "print(confusion_matrix(np.ndarray.flatten(M_tar_s_test[dfindex][:,0:2]), np.ndarray.flatten(np.round(pred_bin))))\n",
    "print(prfs(np.ndarray.flatten(M_tar_s_test[dfindex][:,0:2]), np.ndarray.flatten(np.round(pred_bin))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
